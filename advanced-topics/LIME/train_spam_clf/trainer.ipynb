{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1dbe732",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import sklearn\n",
    "import pickle\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,StratifiedKFold,cross_val_score,learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa30f9",
   "metadata": {},
   "source": [
    "## Preprocessing and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fbcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/spam.csv', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65bd39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing unwanted columns\n",
    "\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v2\" : \"text\", \"v1\":\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d54d6489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mc15445/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mc15445/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import nltk packages and Punkt Tokenizer Models\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b4aa1",
   "metadata": {},
   "source": [
    "## WordClouds- to see which words are common in SPAM and NOT SPAM mesaages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6752334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_words = ''\n",
    "spam_words = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319e2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus of spam messages\n",
    "for val in data[data['label'] == 'spam'].text:\n",
    "    text = val.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for words in tokens:\n",
    "        spam_words = spam_words + words + ' '\n",
    "\n",
    "# Creating a corpus of ham messages\n",
    "for val in data[data['label'] == 'ham'].text:\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for words in tokens:\n",
    "        ham_words = ham_words + words + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13128990",
   "metadata": {},
   "source": [
    "## Creating Spam wordcloud and ham wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8afb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_wordcloud = WordCloud(width=500, height=300).generate(spam_words)\n",
    "ham_wordcloud = WordCloud(width=500, height=300).generate(ham_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9e2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(['ham','spam'],[0, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ebbc8",
   "metadata": {},
   "source": [
    "## Remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2c6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the punctuations and stopwords\n",
    "import string\n",
    "def text_process(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8425eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6471f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.DataFrame(data['text'])\n",
    "label = pd.DataFrame(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba86c3f",
   "metadata": {},
   "source": [
    "## Converting words to vectors using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caaeafee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in data set:  11305\n"
     ]
    }
   ],
   "source": [
    "## Counting how many times a word appears in the dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "total_counts = Counter()\n",
    "for i in range(len(text)):\n",
    "    for word in text.values[i][0].split(\" \"):\n",
    "        total_counts[word] += 1\n",
    "\n",
    "print(\"Total words in data set: \", len(total_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4e7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', '2', 'call', 'U', 'get', 'Im', 'ur', '4', 'ltgt', 'know', 'go', 'like', 'dont', 'come', 'got', 'time', 'day', 'want', 'Ill', 'lor', 'Call', 'home', 'send', 'going', 'one', 'need', 'Ok', 'good', 'love', 'back', 'n', 'still', 'text', 'im', 'later', 'see', 'da', 'ok', 'think', 'ÃŒ', 'free', 'FREE', 'r', 'today', 'Sorry', 'week', 'phone', 'mobile', 'cant', 'tell', 'take', 'much', 'night', 'way', 'Hey', 'reply', 'work', 'make', 'give', 'new']\n"
     ]
    }
   ],
   "source": [
    "# Sorting in decreasing order (Word with highest frequency appears first)\n",
    "vocab = sorted(total_counts, key=total_counts.get, reverse=True)\n",
    "print(vocab[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32d28bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from words to index\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {}\n",
    "#print vocab_size\n",
    "for i, word in enumerate(vocab):\n",
    "    word2idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346d3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to Vector\n",
    "def text_to_vector(text):\n",
    "    word_vector = np.zeros(vocab_size)\n",
    "    for word in text.split(\" \"):\n",
    "        if word2idx.get(word) is None:\n",
    "            continue\n",
    "        else:\n",
    "            word_vector[word2idx.get(word)] += 1\n",
    "    return np.array(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33b942c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 11305)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all titles to vectors\n",
    "word_vectors = np.zeros((len(text), len(vocab)), dtype=np.int_)\n",
    "for i, (_, text_) in enumerate(text.iterrows()):\n",
    "    word_vectors[i] = text_to_vector(text_[0])\n",
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a7f29",
   "metadata": {},
   "source": [
    "## Converting words to vectors using TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fccb6f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9376)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the text data into vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(data['text'])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f996bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = word_vectors\n",
    "features = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e04d5",
   "metadata": {},
   "source": [
    "## Splitting into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f8e3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['label'], test_size=0.15, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc5c9f",
   "metadata": {},
   "source": [
    "## Classifying using sklearn pre built classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afacf963",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3972859197.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [20]\u001b[0;36m\u001b[0m\n\u001b[0;31m    pickle.dump(p1, f)rb') as f:\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# train model\n",
    "clf = MultinomialNB(alpha=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "# save model pipeline\n",
    "import pickle\n",
    "from sklearn.pipeline import make_pipeline\n",
    "p1 = make_pipeline(vectorizer, clf)\n",
    "with open('../spam-clf.pkl', 'wb') as f:\n",
    "    pickle.dump(p1, f)rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
