# Project 15: Visual Search using Deep Embeddings 

In the fashion industry, clothing items are often photographer from different angles (see ASOS dresses for example). Snap Vision enables someone browsing a fashion web site to use visual search to discover new items of clothing that are similar to a chosen target item. This visual search can be accomplished by comparing embedded encodings of the relevant images. However, these embedded encodings tend to be viewpoint dependent, i.e., embeddings of the same item of clothing will be significantly different for images taken from different angles which can interfere with visual search. Recently, so-called “deep embedding” techniques have been developed to maintain similarity across multiple views.  

This project will work with a dataset comprising thousands of four-view image sets supplied by Snap Vision. The target is to develop and explore a pipeline that allows deep embeddings to be automatically learned from image data and used to drive visual search that can cope with multiple views.  


