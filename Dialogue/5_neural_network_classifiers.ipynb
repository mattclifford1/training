{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Neural Networks for Text Classification\n",
    "\n",
    "This lab introduces (deep) neural networks for text classification using Pytorch, and applies it to the datasets we previously used with naïve Bayes and logistic regression.\n",
    "\n",
    "You may also find [Pytorch's tutorials](https://pytorch.org/tutorials/) useful.\n",
    "\n",
    "### Outcomes\n",
    "* Be able to construct and train a neural network classifier in Pytorch.\n",
    "* Understand how to use word embeddings as input to a neural network.\n",
    "* Know how to compare classifier performance on a test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the Data\n",
    "\n",
    "This section contains the same loader code as last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2938f2c1d1f49a18c545c9f1d9e8b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103f639a9e9b45a5b15260273a206d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 45615 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development/validation dataset with 2000 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tweet_eval (./data_cache/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset with 12284 instances loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45615/45615 [00:05<00:00, 8618.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"train\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "dev_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"validation\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Development/validation dataset with {len(dev_dataset)} instances loaded\")\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    split=\"test\",\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Put the data into lists ready for the next steps...\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_texts.append(train_dataset[i]['text'])\n",
    "    train_labels.append(train_dataset[i]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparing the Data\n",
    "Now we put the dataset into a suitable format for a Pytorch NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As inputs to the Sklearn classifiers in week 3, we used CountVectorizer to extract a single vector representation for a *whole document*.\n",
    "However, one motivation for using a neural network is that it can learn to combine the tokens automatically, so we don't need to pass in a document vector.\n",
    "Instead, as input to our neural network, we will pass in each token as its *input_id*, which is its index into a vocabulary.\n",
    "\n",
    "The first step is to compute the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mc15455/anaconda3/envs/general38/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizer to obtain a vocabulary\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize)  # use the Gensim tokenizer so we get consistency with the Gensim embeddings\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41235\n",
      "41236\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(np.max(list(vocab.values())))\n",
    "print(len(vocab))\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to map the tokens to their IDs -- their indexes in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4783e28ccd0140fba90792411541bea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45615 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize training set and convert to input IDs.\n",
    "def encode_text(sample):\n",
    "    tokens = tokenize(sample['text'])\n",
    "    input_ids = []\n",
    "    for token in tokens:\n",
    "        if str.lower(token) in vocab:  # Skip words from the dev/test set that are not in the vocabulary.\n",
    "            input_ids.append(vocab[str.lower(token)]+1)\n",
    "    sample['input_ids'] = np.array(input_ids)  # +1 is needed because we reserve 0 as a special character\n",
    "    return sample\n",
    "\n",
    "train_dataset = train_dataset.map(encode_text)\n",
    "# for i in tqdm(range(len(train_dataset))):\n",
    "#     train_dataset[i]['input_ids'] = encode_text(train_dataset[i])\n",
    "# print(train_dataset[0])\n",
    "# sample = encode_text(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"', 'label': 2, 'input_ids': [28858, 38435, 17515, 36182, 26061, 10434, 25593, 36182, 36125, 4205, 29980, 21587, 35199, 36182, 2947, 25593, 16479, 15672]}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "# print(encode_text(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network's input layer has a fixed size, so we need to somehow make all of our documents have the same number of tokens. We can do this by setting a fixed sequence length, then *padding* the short documents with a special token. Any documents that exceed the length will be truncated. Let's plot a histogram to understand the length distribution of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the document length: 19.684752822536446\n",
      "Median of the document length: 20.0\n",
      "Maximum document length: 46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([5.0000e+01, 1.3000e+03, 6.5510e+03, 1.0287e+04, 1.6098e+04,\n",
       "        8.6200e+03, 2.5290e+03, 1.6100e+02, 1.8000e+01, 1.0000e+00]),\n",
       " array([ 1. ,  5.5, 10. , 14.5, 19. , 23.5, 28. , 32.5, 37. , 41.5, 46. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rv_l = [len(doc) for doc in train_dataset['input_ids']]\n",
    "print('Mean of the document length: {}'.format(np.mean(rv_l)))\n",
    "print('Median of the document length: {}'.format(np.median(rv_l)))\n",
    "print('Maximum document length: {}'.format(np.max(rv_l)))\n",
    "\n",
    "plt.hist(rv_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 2.1** Modify the padding code below to insert 0s at the start of any sequences that are too short, and to truncate any sequences that are too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75142c4a051540f7bf842646695a2499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45615 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_length = 40  # truncate all docs longer than this. Pad all docs shorter than this.\n",
    "\n",
    "def pad_text(sample):\n",
    "    sequence_length = 40\n",
    "    # pad \n",
    "    sample['input_ids'] = np.pad(sample['input_ids'], (0, sequence_length))\n",
    "    sample['input_ids'] = sample['input_ids'][:sequence_length]\n",
    "    return sample\n",
    "\n",
    "train_dataset = train_dataset.map(pad_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45615/45615 [00:04<00:00, 10942.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check it worked\n",
    "result = 'all good'\n",
    "for example in tqdm(train_dataset):\n",
    "    if len(example['input_ids']) != sequence_length:\n",
    "        result = 'fail'\n",
    "        break\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our data in the right format. When training, the neural network will process the data in randomly-chosen mini-batches.\n",
    "To enable this, we wrap our dataset in a DataLoader\n",
    "\n",
    "DataLoader: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(dataset, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(np.array(dataset['input_ids']))\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = len(np.unique(train_labels))   # number of possible labels in the sentiment analysis task\n",
    "\n",
    "train_loader = convert_to_data_loader(train_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the development and test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530f91dd00c741e8b99d948d8e50c42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1cdb7d6930458795c50a5cc5e1f2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c337e43b982e4a7c9f0f2db0ebe5c790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12284 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7d6b10807a4388984a2c642ae6c484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12284 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dev_dataset = dev_dataset.map(encode_text)\n",
    "dev_dataset = dev_dataset.map(pad_text)\n",
    "dev_loader = convert_to_data_loader(dev_dataset, num_classes)\n",
    "\n",
    "test_dataset = test_dataset.map(encode_text)\n",
    "test_dataset = test_dataset.map(pad_text)\n",
    "test_loader = convert_to_data_loader(test_dataset, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Constructing the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a NN with three different layers for sentiment classification.\n",
    "\n",
    "### Embedding layer\n",
    "In the embedding layer, the network will create its own embeddings for the index with a given embedding dimension.\n",
    "The module `nn.Embedding()` creates a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "This module is often used to store word embeddings and retrieve them using indices.\n",
    "The module's input is a list of indices, and the output is the corresponding word embeddings.\n",
    "\n",
    "[Documentation for Embedding Class](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "\n",
    "### Fully-connected layer\n",
    "Fully-connected layers in a neural network are those layers where all the inputs from the previous layer are connected to every unit of the fully-connected layer.\n",
    "Here we will use fully-connected layers for the hidden layer and output layer. In Pytorch this kind of layer is implemented by the 'Linear' class:\n",
    "\n",
    "[Documentation for Linear Class](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "## Activation functions\n",
    "In Pytorch, the activation function is not included in the Linear class (or other kinds of neural network layer).\n",
    "In Pytorch, we construct a neural network by connecting up the output of each component to the input of the next, thereby creating a computation graph.\n",
    "To complete the hidden layer, we connect the ouput of a linear layer to a ReLU activation function, thereby creating a nonlinear function.\n",
    "\n",
    "**TODO 3.1** Complete the constructor for a NN with three layers by adding the missing dimensions.\n",
    "\n",
    "**TODO 3.2** Complete the forward function that maps the input data to an output by adding the missing line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        ### COMPLETE THE CODE HERE: WRITE IN THE MISSING ARGUMENTS SPECIFYING THE DIMENSIONS OF EACH LAYER\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "        self.hidden_layer = nn.Linear(sequence_length*embedding_size, hidden_size) # Hidden layer\n",
    "        self.activation = nn.ReLU() # Hidden layer\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Full connection layer\n",
    "\n",
    "        ##########\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        embedded_words = embedded_words.reshape(embedded_words.shape[0], sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "        z = self.hidden_layer(embedded_words)   # (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        ### ADD THE MISSING LINES HERE\n",
    "        h = self.activation(z)\n",
    "        ########\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output\n",
    "\n",
    "vocab_size = len(vectorizer.vocabulary_) + 1\n",
    "embedding_size = 25  # number of dimensions for embeddings\n",
    "hidden_size = 32 # number of hidden units\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n",
    "\n",
    "model = FFTextClassifier(vocab_size, embedding_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.3** Create a NN with the FFTextClassifier class we wrote.\n",
    "\n",
    "**Hint:** `model = FFTextClassifier(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vectorizer.vocabulary_) + 1\n",
    "embedding_size = 25  # number of dimensions for embeddings\n",
    "hidden_size = 32 # number of hidden units\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n",
    "\n",
    "model = FFTextClassifier(vocab_size, embedding_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After desigining our network, we need to create a training function to calculate the loss for each input and perform backpropagation to optimise the network.\n",
    "During training, the weights of all the layer will be updated.\n",
    "\n",
    "We build a training function to train the NN over a fixed number of epochs (an epoch is one iteration over the whole training dataset).\n",
    "The function also prints the performance of both training and development/validation set after each epoch.\n",
    "\n",
    "**TODO 3.4** Complete the code below to compute the validation accuracy and loss after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader, loss_fn, optimizer):\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += batch_labels.size(0)\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        dev_losses = []\n",
    "\n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            ###WRITE YOUR OWN CODE HERE\n",
    "            dev_output = model(dev_input_ids)\n",
    "            #######\n",
    "            dev_loss = loss_fn(dev_output, dev_labels)\n",
    "            # Save the loss on the dev set\n",
    "            dev_losses.append(dev_loss.item())\n",
    "\n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += dev_labels.size(0)\n",
    "            \n",
    "        dev_accuracy = total_correct/total_trained*100\n",
    "        \n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(np.mean(dev_losses)),\n",
    "              \"Validation Accuracy: {:.4f}%\".format(dev_accuracy))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before we start training is defining the loss function and optimizer.\n",
    "\n",
    "Here we use cross-entropy loss and the Adam optimizer (it tends to find a better solution in a small number of iterations than SGD).\n",
    "The module `nn.CrossEntropyLoss()` combines `LogSoftmax` and `NLLLoss` in one single class so that we don't have to implement the softmax layer within the forward() method.\n",
    "\n",
    "Cross Entropy Loss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "Optimization: https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "**TODO 3.4** Finally, train the network for 10 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Training Loss: 0.4886 Training Accuracy: 80.0811%\n",
      "Epoch: 1/10 Validation Loss: 1.0780 Validation Accuracy: 57.6000%\n",
      "Epoch: 2/10 Training Loss: 0.4536 Training Accuracy: 81.7933%\n",
      "Epoch: 2/10 Validation Loss: 1.1092 Validation Accuracy: 58.0000%\n",
      "Epoch: 3/10 Training Loss: 0.4190 Training Accuracy: 83.1985%\n",
      "Epoch: 3/10 Validation Loss: 1.1539 Validation Accuracy: 57.7500%\n",
      "Epoch: 4/10 Training Loss: 0.3872 Training Accuracy: 84.9457%\n",
      "Epoch: 4/10 Validation Loss: 1.2052 Validation Accuracy: 58.8500%\n",
      "Epoch: 5/10 Training Loss: 0.3587 Training Accuracy: 86.0660%\n",
      "Epoch: 5/10 Validation Loss: 1.2763 Validation Accuracy: 57.0000%\n",
      "Epoch: 6/10 Training Loss: 0.3303 Training Accuracy: 87.5041%\n",
      "Epoch: 6/10 Validation Loss: 1.3009 Validation Accuracy: 58.0500%\n",
      "Epoch: 7/10 Training Loss: 0.3053 Training Accuracy: 88.5301%\n",
      "Epoch: 7/10 Validation Loss: 1.4041 Validation Accuracy: 57.9000%\n",
      "Epoch: 8/10 Training Loss: 0.2800 Training Accuracy: 89.7446%\n",
      "Epoch: 8/10 Validation Loss: 1.4441 Validation Accuracy: 58.2000%\n",
      "Epoch: 9/10 Training Loss: 0.2562 Training Accuracy: 90.6325%\n",
      "Epoch: 9/10 Validation Loss: 1.5065 Validation Accuracy: 58.2000%\n",
      "Epoch: 10/10 Training Loss: 0.2364 Training Accuracy: 91.4239%\n",
      "Epoch: 10/10 Validation Loss: 1.5744 Validation Accuracy: 58.6500%\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "###WRITE YOUR OWN CODE HERE\n",
    "trained_model = train_nn(10, \n",
    "         model, \n",
    "         train_loader, \n",
    "         dev_loader, \n",
    "         loss_fn, \n",
    "         optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3.5:** Evaluate the model on test set using the function below. Complete the code to count the correct classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 52.13%\n"
     ]
    }
   ],
   "source": [
    "def test_nn(trained_model, test_loader, loss_fn):\n",
    "\n",
    "    trained_model.eval()\n",
    "\n",
    "    test_losses = []\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        loss = loss_fn(test_output, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        ###WRITE YOUR OWN CODE HERE\n",
    "        for i in range(len(labels)):\n",
    "            if predicted_labels[i]==labels[i]:\n",
    "                correct += 1\n",
    "        ######\n",
    "\n",
    "    accuracy = correct/len(test_loader.dataset)*100\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
    "    # print(predicted)\n",
    "\n",
    "test_nn(trained_model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pretrained Embeddings\n",
    "\n",
    "Now let's use pretrained word embeddings as inputs instead of learning them from scratch during training.\n",
    "Here, we will use a pretrained embedding matrix to initialise the embedding layer, which will then be updated during training.\n",
    "\n",
    "The class below extends the FFTextClassifier class. This means that it inherits all of its functionality, but we now overwrite the constructor (the `__init__` method).\n",
    "This way, we don't need to define the forward function again, as it will be the same as before.\n",
    "\n",
    "**TODO 4.1** As before, complete the arguments below to set the dimensions of the neural network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "class FFTextClassifierWithEmbeddings(FFTextClassifier):\n",
    "\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        # download pretrained embeddings as the intial values for out embedding layer.\n",
    "        embedding_matrix = gensim.downloader.load('glove-twitter-25')  # 'word2vec-google-news-300')\n",
    "        self.embedding_size = embedding_matrix.vectors.shape[1]\n",
    "        embedding_matrix_reordered = torch.zeros((vocab_size, self.embedding_size))\n",
    "        for word in vocab:\n",
    "            if word in embedding_matrix:\n",
    "                embedding_matrix_reordered[vocab[word]] = torch.from_numpy(embedding_matrix[word])\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embedding_matrix_reordered, freeze=False) # embedding layer\n",
    "\n",
    "        ### COMPLETE THE ARGUMENTS TO SPECIFY THE DIMENSIONS OF THE LAYERS\n",
    "        self.hidden_layer = nn.Linear(???) # Hidden layer\n",
    "        self.activation = nn.ReLU() # Hidden layer\n",
    "        self.output_layer = nn.Linear(???) # Full connection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO 4.2** Using the above class, construct, train and test the classifier with pretrained embeddings. You will need to create a new optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR OWN CODE BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
