{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model\n",
    "\n",
    "In this lab we will implement a bigram language model and use it to compute the probability of some sample sentences. \n",
    "\n",
    "### Outcomes\n",
    "* Know how to count word frequencies in a corpus using Python libraries.\n",
    "* Understand how to compute conditional probabilities.\n",
    "* Be able to apply the chain rule to compute the probability of a sentence.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads the same dataset as last week. \n",
    "The next part splits the data into training and test sets, and tokenises the utterances.\n",
    "After this there are some tasks to complete to implement and test the language model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset doc2dial (./data_cache/doc2dial/dialogue_domain/1.0.1/c15afdf53780a8d6ebea7aec05384432195b356f879aa53a4ee39b740d520642)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances: 22151\n"
     ]
    }
   ],
   "source": [
    "# Collect all the utterances into a list. \n",
    "# For this task, we don't care about the order of the utterances in the conversation -- \n",
    "# we will just be using the utterances of examples of the language we want to model.\n",
    "\n",
    "utterances = []\n",
    "for sample in dataset:\n",
    "    turns = sample['turns']\n",
    "    for turn in turns:\n",
    "        if turn['role'] == 'user':\n",
    "            utterances.append(turn['utterance'])\n",
    "            \n",
    "###\n",
    "print(f'Number of utterances: {len(utterances)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thanks', ',', 'and', 'in', 'case', 'I', 'forget', 'to', 'bring', 'all', 'of', 'the', 'documentation', 'needed', 'to', 'the', 'DMV', 'office', ',', 'what', 'can', 'I', 'do', '?']\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the samples. You can replace NLTK with another tokenizer if you prefer. \n",
    "import nltk\n",
    "\n",
    "for i in range(len(utterances)):\n",
    "    utterances[i] = nltk.word_tokenize(utterances[i])\n",
    "    \n",
    "print(utterances[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to put in some artificial start <s> and end <e> tokens. \n",
    "# These will be used to model which words are most likely to start or end a sentence. \n",
    "\n",
    "for i in range(len(utterances)):\n",
    "    utterances[i] = ['<s>'] + utterances[i] + ['<e>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test using scikit-learn.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = 0.8\n",
    "test_size = 0.2\n",
    "\n",
    "# Split the train data from the test data\n",
    "train_data, test_data = train_test_split(utterances, train_size=train_size, test_size=test_size)\n",
    "\n",
    "\n",
    "print(f'The training set has {len(train_data)} samples and the test set has {len(test_data)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting Tokens\n",
    "\n",
    "The n-gram language model needs to compute two sets of counts from the training data:\n",
    "1. The counts of how many times each bigram occurs.\n",
    "2. The counts of how many times each first token (condition in the conditional probability) occurs. \n",
    "\n",
    "Let's start by finding the vocabulary of unique token 'types': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocab = np.unique(np.concatenate(train_data))\n",
    "V = len(vocab)\n",
    "\n",
    "print(vocab)\n",
    "print(f'There are {V} types in our vocabulary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an object to store the bigram counts in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A matrix where row indexes will correspond to the first token in a bigram, \n",
    "# and column indexes to the second token. The indexes must map to the index\n",
    "# of the token in the vocabulary. The values in the matrix will be the counts.\n",
    "counts = np.zeros((V, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to find the index of a given word:\n",
    "\n",
    "word = '<s>'  # example word\n",
    "index = np.argwhere(vocab == word)[0][0]\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 1: count the bigrams that occur in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 2: use numpy's sum() function to compute the first token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 3: compute a matrix (numpy array) of conditional probabilities using the counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 4: write a function that computes the probability of a given tokenised sentence, such as the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example tokenised sentence\n",
    "sen = ['<s>', 'If', 'you', 'give', 'me', 'the', 'help', ',', 'what', 'is', 'the', 'payment', 'system', '?', '<e>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 5: compute the perplexity over the whole test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTENSION 1: use the language model to generate new sentences by sampling. \n",
    "You can follow the example below to sample using scipy's multinomial class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multinomial\n",
    "\n",
    "example_vocab = np.array(['a', 'b', 'c', 'd'])\n",
    "\n",
    "sample = multinomial.rvs(1, [0.3, 0.2, 0.1, 0.4])\n",
    "sample_bool = sample.astype(bool)  # convert the sample from integer to boolean\n",
    "generated_word = example_vocab[sample_bool]  # use the one-hot boolean vector to look up the word\n",
    "\n",
    "print(generated_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE EXTENSIONS: \n",
    "* Add some smoothing to the counts and see how it affects the results.\n",
    "* Use trigrams instead of bigrams. Does it improve perplexity?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
