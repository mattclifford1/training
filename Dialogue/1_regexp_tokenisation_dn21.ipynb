{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Lab\n",
    "\n",
    "### Aims\n",
    "* Install the required libraries and refamiliarise yourself with Python and Jupyter notebooks if you need it\n",
    "* Understand regular expressions\n",
    "* Carry out tokenisation steps\n",
    "\n",
    "### Outline\n",
    "\n",
    "* Getting started: libraries, how to install them, Jupyter notebooks introduction\n",
    "* Acquiring dialogue data\n",
    "* Regular expressions\n",
    "* Tokenisation\n",
    "\n",
    "### How To Complete This Lab\n",
    "\n",
    "Read the text and the code then look for 'TODOs' that instruct you to complete some missing code. You don't have to stick rigidly to the lab -- feel free to explore other methods and data to help you understand what's going on or to go beyond this lab. \n",
    "\n",
    "Aim to work through the lab during the scheduled lab hours. You can also post your questions to our Team's general channel throughout the week.\n",
    "\n",
    "The labs *will not be marked*. However, they will prepare you for the coursework, so try to keep up with the weekly labs and have fun with the exercises!\n",
    "      \n",
    "### Additional Exercises\n",
    "\n",
    "If you would like to do more lab exercises or would like an alternative explanation, please see Chapters 1-3 in the NLTK book, which goes into more detail than we do here. https://www.nltk.org/book/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "This lab assumes you have used Python and Jupyter Notebooks before. For an introduction or refresher on Python, see the [Introduction to Python lab](https://github.com/UoB-COMS21202/lab_sheets_public/tree/master/lab_1) or the University of Bristol [Beginning Python](https://milliams.gitlab.io/beginning_python/) course. If you are a beginner with Python, you might also like to look at Chapter 1 in the NLTK book, which also provides a guide for \"getting started with Python\": https://www.nltk.org/book/ \n",
    "\n",
    "You will need to use Python 3, not Python 2. Specifically Python 3.6 or newer is recommended.\n",
    "\n",
    "The following libraries will be used in this lab. You will need to learn how to install new packages using conda or pip (recommended to use a virtual environment) when they come up in later labs.\n",
    "\n",
    "- [Transformers](https://huggingface.co/transformers/index.html).\n",
    "- [NLTK](https://www.nltk.org/) (optional) OR [Spacy](https://spacy.io/).\n",
    "\n",
    "The libraries above have good documentation which can be used to learn other features of the libraries or for questions and examples. The documentation is available either online (links above) or via Python itself, e.g. `help(numpy.array)` in the Python interpreter.\n",
    "\n",
    "As an example, to install nltk in a new conda environment, run\n",
    "```\n",
    "$ conda create -n DN_labs\n",
    "$ conda activate DN_labs \n",
    "$ conda install nltk\n",
    "```\n",
    "For further help see the installation guides on the libraries documentation.\n",
    "\n",
    "**Feel free to skip the next part if you're already confident with Jupyter notebooks.**\n",
    "\n",
    "## Jupyter Notebook\n",
    "\n",
    "The labs will be run on [Jupyter Notebook](http://jupyter.org/), an interactive coding environment embedded in a webpage supporting various programing languages (Python, R, Lua, etc.) through the concept of kernels.  \n",
    "\n",
    "It allows you to enrich your code with complex comments formatted in Markdown and $\\LaTeX$, as well as to place the results of your computation right below your code.\n",
    "\n",
    "Notebooks are organised in cells which can contain either code (in our case, this will be Python code) or text, which can be easily and nicely formatted using the Markdown notation. \n",
    "\n",
    "To edit an already existing cell simply double-click on it. You can use the toolbar to insert new cells, edit and delete them (or use keyboard shortcuts which are very handy to speed up coding). \n",
    "\n",
    "Cells can be run, by hitting `shift+enter` when editing a cell or by clicking on the `Run` button at the top. Running a Markdown cell will simply display the formatted text, while running a code cell will execute the commands executed in it. \n",
    "\n",
    "**Note**: when you run a code cell, all the created variables, implemented functions and imported libraries will be then available to every other code cell. However, it is commonly assumed that cells will be run sequentially in terms of prerequisites. To reset all variables and functions (for debugging) simply click `Kernel > Restart` from the Jupyter menu.\n",
    "\n",
    "#### A bit on Markdown language (and a bit of LaTeX and HTML) if you're interested\n",
    "\n",
    "Markdown cells allow you to write fancy and simple comments: all of this is written in Markdown - double click on this cell to see the source. Introduction to Markdown syntax can be found [here](https://daringfireball.net/projects/markdown/syntax).\n",
    "\n",
    "As Markdown is translated to HTML upon displaying it also allows you to use pure HTML: more details are available [here](https://daringfireball.net/projects/markdown/syntax#html).\n",
    "\n",
    "Finally, you can also display simple $\\LaTeX$ equations in Markdown thanks to `MathJax` support. For inline equations wrap your equation between `$` symbols; for display mode equations use `$$`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Doc2Dial Dataset\n",
    "\n",
    "This is a recent 'shared task' that involves building a dialog system. The goal is to respond to a user by first retrieving some information from a document, then using it to formulate a response. More on the task here:\n",
    "https://doc2dial.github.io/workshop2021/shared.html\n",
    "\n",
    "The raw data is available [here](https://doc2dial.github.io/file/doc2dial_v1.0.1.zip) but we will use a data loader class from the [HuggingFace datasets library](https://huggingface.co/docs/datasets/loading_datasets.html) to load it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset doc2dial (./data_cache/doc2dial/dialogue_domain/1.0.1/c15afdf53780a8d6ebea7aec05384432195b356f879aa53a4ee39b740d520642)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"\n",
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"doc2dial\",\n",
    "    name=\"dialogue_domain\",  # this is the name of the dataset for the second subtask, dialog generation\n",
    "    split=split,\n",
    "    ignore_verifications=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 3474 instances\n",
      "An example instance: \n",
      "{'dial_id': '92db4f3c68ab3fb2851f4a559d9c2d1e', 'doc_id': 'Temporary Disability Rating After Surgery Or Cast | Veterans Affairs#1_0', 'domain': 'va', 'turns': [{'turn_id': 1, 'role': 'user', 'da': 'query_condition', 'references': [{'sp_id': '28', 'label': 'precondition'}], 'utterance': 'Hello I need information on How do I get these disability rating benefits?'}, {'turn_id': 2, 'role': 'agent', 'da': 'respond_solution', 'references': [{'sp_id': '29', 'label': 'solution'}, {'sp_id': '30', 'label': 'solution'}], 'utterance': 'To get these benefits, you must file a disability compensation claim. Find out how to file a disability compensation claim'}, {'turn_id': 3, 'role': 'agent', 'da': 'query_condition', 'references': [{'sp_id': '36', 'label': 'precondition'}], 'utterance': 'How much do you have a qualified injury of 30% or more that has worsened?'}, {'turn_id': 4, 'role': 'user', 'da': 'response_negative', 'references': [{'sp_id': '36', 'label': 'precondition'}], 'utterance': 'do not'}, {'turn_id': 5, 'role': 'user', 'da': 'query_solution', 'references': [{'sp_id': '3', 'label': 'solution'}, {'sp_id': '4', 'label': 'solution'}], 'utterance': 'if a veteran has an immobilizing disability how is it rated?'}, {'turn_id': 6, 'role': 'agent', 'da': 'respond_solution', 'references': [{'sp_id': '3', 'label': 'solution'}, {'sp_id': '4', 'label': 'solution'}], 'utterance': 'You may be able to get a 100% temporary disability rating and disability compensation or benefits if you have this type of immobilizing disability.'}, {'turn_id': 7, 'role': 'user', 'da': 'query_solution', 'references': [{'sp_id': '8', 'label': 'solution'}, {'sp_id': '9', 'label': 'solution'}], 'utterance': 'Can I get VA disability benefits?'}, {'turn_id': 8, 'role': 'agent', 'da': 'respond_solution', 'references': [{'sp_id': '8', 'label': 'solution'}, {'sp_id': '9', 'label': 'solution'}], 'utterance': 'Yes, but if you had surgery, you must comply with the following:\\n\\xa0\\xa0\\xa0\\xa0 The surgery required a recovery time of at least one month or reports show that the surgery or treatment was for a service-related disability,'}, {'turn_id': 9, 'role': 'user', 'da': 'query_solution', 'references': [{'sp_id': '11', 'label': 'solution'}, {'sp_id': '12', 'label': 'solution'}, {'sp_id': '13', 'label': 'solution'}], 'utterance': 'What if problems arose during surgery?'}, {'turn_id': 10, 'role': 'agent', 'da': 'respond_solution', 'references': [{'sp_id': '11', 'label': 'solution'}, {'sp_id': '12', 'label': 'solution'}, {'sp_id': '13', 'label': 'solution'}], 'utterance': 'If Surgery resulted in serious problems, such as: not being completely healed or Surgical wounds can benefit'}, {'turn_id': 11, 'role': 'user', 'da': 'query_solution', 'references': [{'sp_id': '6', 'label': 'solution'}], 'utterance': 'Can I get VA disability benefits?'}, {'turn_id': 12, 'role': 'agent', 'da': 'respond_solution', 'references': [{'sp_id': '6', 'label': 'solution'}], 'utterance': 'You may be able to get disability benefits if you had surgery or other treatment at a VA hospital, an approved hospital, or an outpatient facility for a military-related disability called service-related disability'}]}\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataset has {len(dataset)} instances')\n",
    "\n",
    "print('An example instance: ')\n",
    "print(dataset[2342])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we can access the different fields in a single data sample as if reading from a Python dictionary.\n",
    "\n",
    "For our lab this week, we will need some examples of dialogue written by a user. Let's get some from the training set of Doc2Dial.\n",
    "\n",
    "TODO 2.1: get a list of user utterances from 100 different conversations. Name the list 'utterances'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "['Hello, I forgot o update my address, can you help me with that?', 'hi, you have to report any change of address to DMV within 10 days after moving. You should do this both for the address associated with your license and all the addresses associated with all your vehicles.', 'Can I do my DMV transactions online?', 'Yes, you can sign up for MyDMV for all the online transactions needed.', 'Thanks, and in case I forget to bring all of the documentation needed to the DMV office, what can I do?', \"This happens often with our customers so that's why our website and MyDMV are so useful for our customers. Just check if you can make your transaction online so you don't have to go to the DMV Office.\", 'Ok, and can you tell me again where should I report my new address?', \"Sure. Any change of address must be reported to the DMV, that's for the address associated with your license and any of your vehicles.\", 'Can you tell me more about Traffic points and their cost?', 'Traffic points is the system used by DMV to track dangerous drivers. The cost of the traffic points is independent of the DRA, so you get a separate charge based on the total points you accumulate.']\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR ANSWER HERE\n",
    "utterances = []\n",
    "num_of_examples = 100\n",
    "for data_entry in dataset:\n",
    "    for turn in data_entry['turns']:\n",
    "        if len(utterances) >= num_of_examples:\n",
    "            break\n",
    "        elif 'utterance' in turn.keys():\n",
    "            utterances.append(turn['utterance'])\n",
    "###\n",
    "utterances.append('we are going to Canada canada tomorrow.')\n",
    "print(len(utterances))\n",
    "print(utterances[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regular Expressions\n",
    "\n",
    "Next, we are going to experiment with building a simple chatbot using regular expressions. The aims are to get familiar with this important NLP tool and to see some limitations of rule-based approaches.\n",
    "\n",
    "Many text processing systems make use of regular expressions, which are a language for specifying sets of strings. We can use regular expressions to define a pattern to search for in a corpus of text and retrieve all the occurrences of that pattern. We can also use regular expressions to replace on text pattern with another. Regular expressions are therefore used in various NLP systems, e.g., to implement tokenisation or extract features for a classifier by looking for specific patterns. They can often be used to build a simple baseline for tasks like text classification before developing a machine learning solution. \n",
    "\n",
    "## 3.1 Search\n",
    "\n",
    "We can start by finding occurrences of the word \"inform\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can'}\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def find_re(word_to_find, utterances, print_set=True):\n",
    "    all_matches = []\n",
    "    for utterance in utterances:\n",
    "        matches = re.findall(word_to_find, utterance)\n",
    "        if len(matches):  # if it found something\n",
    "            all_matches.extend(matches)\n",
    "    if print_set:\n",
    "        print(set(all_matches))  # use a set to get the unique instances in the list\n",
    "        print(len(all_matches))  # length of the list of matches\n",
    "    return all_matches\n",
    "        \n",
    "    \n",
    "word_to_find = r'can'\n",
    "all_matches = find_re(word_to_find, utterances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just searching for the word itself does not really use the power of regular expressions. Let's use the *disjunction* capabilities of REs to find both capitalised and lower case occurrences. The disjunction of two or more characters is written using square brackets. To match either 'C' or 'c', we can use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can', 'can'}\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "word_to_find = r'[Cc]an'\n",
    "all_matches = find_re(word_to_find, utterances)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current search does not consider word boundaries, so will retrieve occurrences of \"can\" that are part of a longer word. In Python regular expressions, we can match word boundaries using '\\b'. This matches the empty string at the beginning or end of a word. \n",
    "\n",
    "TODO: Write a new regular expression to search for \"can\" that excludes words like \"canal\" and \"arcane\" using '\\b':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can', 'can'}\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "word_to_find = r'\\b[Cc]an\\b'\n",
    "all_matches = find_re(word_to_find, utterances)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of other special characters besides '\\b'. For a complete list of special characters, see https://docs.python.org/3/library/re.html#regular-expression-syntax. For the next exercise, the ones you will need are:\n",
    "   * Match any lower case letter: 'a-z'\n",
    "   * Repetition: Match zero or more repetitions of the preceding RE: '\\*'\n",
    "   * Disjunction between longer expressions: Match either the RE on the left-hand side OR the RE on the right-hand side: '\\|'\n",
    "   * Groups: '(...)'. Parentheses encapsulate *groups*, which are nested regular expressions within a larger RE. They are useful because you can apply special characters such as \\* and \\| to expressions inside a group. If we specify an RE containing N groups, each match returned by findall will be a list of length N where each item corresponds to a group.\n",
    "\n",
    "These special characters can implement two other ways to find both upper and lower case occurrences of 'can': r'Cear|cear' or r'(C|c)(ear)'. The second will divide each match up into two *groups*, where each group matches one of the expressions inside the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can', 'can'}\n",
      "32\n",
      "{('c', 'an'), ('C', 'an')}\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "all_matches = find_re(r'Can|can', utterances)\n",
    "all_matches = find_re(r'(C|c)(an)', utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to concatenate the groups of characters back together to retrieve the complete matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Can', 'can'}\n"
     ]
    }
   ],
   "source": [
    "matches = [m[0] + m[1] for m in all_matches]\n",
    "print(set(matches))  # set() extracts the unique items in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand what's happening in the cell above.\n",
    "* Why do we need the parentheses around 'ear' in the RE in the cell above? \n",
    "* What happens if we remove the parentheses?\n",
    "* On the first line in the cell above, what do the square brackets '[...]' do? Hint: *list comprehension* \n",
    "\n",
    "Now, let's use the special characters described above to retrieve all the words containing 'can'. \n",
    "\n",
    "TODO: Write a new regular expression than returns all complete words starting with \"can\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('c', 'an', 'ada'), ('c', 'an', ''), ('c', 'an', 'cel'), ('C', 'an', 'ada'), ('c', 'an', 'celing'), ('C', 'an', '')}\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "all_matches = find_re(r'\\b(C|c)(an)([a-z]*)', utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to seem more useful -- we've retrieved a set of related words with a common substring. What we really want is to extract the whole context of these words, i.e., the sentences or phrases they are contained in. For this we need a few more special characters:\n",
    "   * Any character except newline: '.'\n",
    "   * Complement, match any character except the specified ones: '[^A]'\n",
    "   * New line: '\\\\n'\n",
    "   * Escape: e.g., '\\\\?', '\\\\'. Using the backslash in front of special characters means that they are not interpreted as special chracters but are treated literally, in this case as a question mark or full stop. \n",
    "\n",
    "TODO: the code below retrieves all phrases including 'can', starting from the preceding punctuation mark or newline, until the following punctuation mark or newline. Modify it so it does not retrieve words like 'Canada' that contain 'can'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what can I do\n",
      " you can sign up for MyDMV for all the online transactions needed\n",
      " you could check on the DMV website to see if you can do your transaction online\n",
      " The best you can do is to check our website to see if you can do your transaction online so you don't have to go to the DMV Office\n",
      " before going to the DMV office you should check our website to see if you can do your transaction online\n",
      " What can I do\n",
      " can you help me with that\n",
      " in thşs case you can miss a suspensin order\n",
      " Before going to a DMV Office you should see if your transaction can be performed online\n",
      " we recommend that you go first to our website to see if you can do your transaction online\n",
      " This way you can avoid going to a DMV Office\n",
      " Can you explain more\n",
      " and can you tell me again where should I report my new address\n",
      " Just check if you can make your transaction online so you don't have to go to the DMV Office\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "all_matches = find_re(r'([\\.\\!\\?\\n;:,])([^\\.\\?!;:,\\n]*)([Cc]an)([^\\.\\?!;:,\\n]*)([\\.\\!\\?\\n;:,])', utterances, print_set=False)\n",
    "        \n",
    "all_matches = [m[1] + m[2] + m[3] for m in all_matches]\n",
    "for match in set(all_matches):\n",
    "    print(match)\n",
    "# print(set(all_matches))  # use a set to get the unique instances in the list\n",
    "print(len(all_matches))  # length of the list of matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Substitution\n",
    "\n",
    "Finding and replacing patterns is an important application of regular expressions. We can use this to clean up the  text we extracted from the book by removing the line breaks, which don't carry much information and are mainly for formatting the EBook.\n",
    "\n",
    "In Python, we can use the re.sub() function to replace one regular expression with an other. re.sub() takes three arguments: \n",
    "* The first argument specifies the expression to match\n",
    "* The second defines the pattern we should replace it with\n",
    "* The third is the text to apply the subtitution to. \n",
    "\n",
    "The *groups* matched by the first argument are assigned to variables that can be referred to by number: \\1 for the first group, \\2 for the second, etc. In the second argument, we can refer to these groups using \\1 to refer to the first group, \\2 to refer to the second group, and so on. \n",
    "\n",
    "The example below shows how to use substitution to remove line breaks from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use regular expression substitutions to create our first dialogue system! A famous chatbot that uses regular expression subsitutions is ELIZA [1], which mimicked a Rogerian psycotherapist using regular expressions.\n",
    "\n",
    "[1] Weizenbaum, J. (1966).  ELIZA – A computer program forthe study of natural language communication between manand machine.CACM 9(1), 36–45\n",
    "\n",
    "In our next task, we are going to use regular expression substition to respond to the dialogue that we extracted from the book. You might need a bit of imagination here: imagine that the dialogue we extracted has been typed into a chatbot and the chatbot must respond suitably.\n",
    "\n",
    "Consider the example below, which uses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIALOGUE:  can you help me with that\n",
      "CHATBOT RESPONSE: Yes, I can help me with that\n",
      "DIALOGUE:  you can sign up for MyDMV for all the online transactions needed\n",
      "CHATBOT RESPONSE: Can I sign up for MyDMV for all the online transactions needed?\n",
      "DIALOGUE:  what can I do\n",
      "CHATBOT RESPONSE: I do not understand.\n",
      "DIALOGUE:  Just check if you can make your transaction online so you don't have to go to the DMV Office\n",
      "CHATBOT RESPONSE: Can I make your transaction online so you don't have to go to the DMV Office?\n",
      "DIALOGUE:  and can you tell me again where should I report my new address\n",
      "CHATBOT RESPONSE: Yes, I can tell me again where should I report my new address\n",
      "DIALOGUE:  The best you can do is to check our website to see if you can do your transaction online so you don't have to go to the DMV Office\n",
      "CHATBOT RESPONSE: Can I do your transaction online so you don't have to go to the DMV Office?\n",
      "DIALOGUE:  you could check on the DMV website to see if you can do your transaction online\n",
      "CHATBOT RESPONSE: Can I do your transaction online?\n",
      "DIALOGUE:  we recommend that you go first to our website to see if you can do your transaction online\n",
      "CHATBOT RESPONSE: Can I do your transaction online?\n",
      "DIALOGUE:  before going to the DMV office you should check our website to see if you can do your transaction online\n",
      "CHATBOT RESPONSE: Can I do your transaction online?\n",
      "DIALOGUE:  Before going to a DMV Office you should see if your transaction can be performed online\n",
      "CHATBOT RESPONSE: I do not understand.\n",
      "DIALOGUE:  This way you can avoid going to a DMV Office\n",
      "CHATBOT RESPONSE: Can I avoid going to a DMV Office?\n",
      "DIALOGUE:  What can I do\n",
      "CHATBOT RESPONSE: I do not understand.\n",
      "DIALOGUE:  in thşs case you can miss a suspensin order\n",
      "CHATBOT RESPONSE: Can I miss a suspensin order?\n",
      "DIALOGUE:  Can you explain more\n",
      "CHATBOT RESPONSE: Yes, I can explain more\n"
     ]
    }
   ],
   "source": [
    "for m in all_matches:   # matches is the list of phrases generated in your previous code cell\n",
    "    \n",
    "    # pretend that each match is an utterance from a user. The dialogue system must generate a response.\n",
    "    print('DIALOGUE: ' + m)    \n",
    "    \n",
    "    # generate responses\n",
    "    search_re = r'.*[Cc]an you'\n",
    "    if re.search(search_re, m):  # repond to the lines that say 'I fear...'   \n",
    "        subbed = re.sub(search_re + r'(.*)', r'Yes, I can\\1', m)\n",
    "    elif re.search(r'.*[Yy]ou can', m):\n",
    "        subbed = re.sub(r'.*[Yy]ou can' + r'(.*)', r'Can I\\1?', m)\n",
    "    else:  # respond to other lines\n",
    "        subbed = 'I do not understand.'\n",
    "    print('CHATBOT RESPONSE: ' + subbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Choose some more patterns to respond to to reduce the frequency with which the chatbot says 'I do not understand' or otherwise improve the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tokenisation\n",
    "\n",
    "Up to now we have been able to work directly with the raw text. However, for most text processing tasks we will need to perform a number of steps to transform the raw text to a suitable format for a model such as a classifier or dialogue system. Here we will try out the key step of word tokenisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a naïve approach: splitting the sentences based on whitespace. \n",
    "\n",
    "The re module provides the re.split() function, which takes a regular expression as its argument and splits the text when it finds a match. The special character '\\s' is used to match whitespace characters.\n",
    "\n",
    "TODO: use re.split() to split the raw text into tokens on whitespace characters. Save the sequence of tokens to a new variable called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'I', 'want', 'to', 'register', 'my', 'vehicle', 'here', 'in', 'new', 'york,', 'I', 'was', 'forewarned', 'that', 'out-of-state', 'insurance', \"can't\", 'be', 'accepted?', '']\n"
     ]
    }
   ],
   "source": [
    "# The dataset has already stripped out most punctuation, so here's a made-up example:\n",
    "raw = \"If I want to register my vehicle here in new york, I was forewarned that out-of-state insurance can't be accepted? \"\n",
    "\n",
    "### WRITE YOUR OWN CODE HERE\n",
    "tokens = re.split(' ', raw)\n",
    "###\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitespace tokenisation doesn't handle things like punctuation very well. For example, parentheses '()' are not excluded from the tokens. To see this, run the following code to inspect the non-letter characters in your tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "york,\n",
      "out-of-state\n",
      "can't\n",
      "accepted?\n"
     ]
    }
   ],
   "source": [
    "for tok in tokens:\n",
    "    if re.search(r'[^a-zA-Z0-9]', tok):\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start to split the tokens based on any non-letter characters, we can encounter further issues. The punctuation may be informative, so we should not throw it away. Hyphenated words may need to be kept together while contractions like \"don't\" might need to be split.\n",
    "\n",
    "Luckily, we can make use of existing rule-based tokenizers that deal with these issues:\n",
    "* Spacy: https://spacy.io/api/tokenizer\n",
    "* NLTK: https://www.kite.com/python/docs/nltk.word_tokenize \n",
    "\n",
    "For some domains and languages, tokenisation is not so easy and we may need to construct a regular-experession based approach.\n",
    "\n",
    "TODO: refer to the documentation linked above for Spacy or NLTK's word tokeniser, and apply one of them to the raw text. Compare the output to the whitespace tokeniser. Save the tokens to a variable called 'tokens_rulebased'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'I', 'want', 'to', 'register', 'my', 'vehicle', 'here', 'in', 'new', 'york', ',', 'I', 'was', 'forewarned', 'that', 'out-of-state', 'insurance', 'ca', \"n't\", 'be', 'accepted', '?']\n"
     ]
    }
   ],
   "source": [
    "### WRITE YOUR OWN CODE HERE\n",
    "import nltk\n",
    "# nltk.download('punkt') # need to downlaod this on first time running\n",
    "tokens_rulebased = nltk.word_tokenize(raw)\n",
    "print(tokens_rulebased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Run the code below to see how NLTK has handled the non-letter characters. What does it do with most punctuation marks? When does it include punctuation marks in a token with letters? When does it not split tokens based on punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "out-of-state\n",
      "n't\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for tok in tokens_rulebased:\n",
    "    if re.search(r'[^a-zA-Z0-9]', tok):\n",
    "        print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the textbook, we also encountered subword tokenization methods, including byte-pair encoding (BPE). We can test this out using the implementation from HuggingFace's Transformers library:\n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "bpe_tokenizer= GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokens_bpe = bpe_tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Print out some of the tokens and see if you can find any subwords. \n",
    "\n",
    "There will be some strange symbols that encode whitespaces, which are treated as part of the following word. See if you can work out what they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I want to register my vehicle here in new york, I was forewarned that out-of-state insurance can't be accepted? \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'ĠI',\n",
       " 'Ġwant',\n",
       " 'Ġto',\n",
       " 'Ġregister',\n",
       " 'Ġmy',\n",
       " 'Ġvehicle',\n",
       " 'Ġhere',\n",
       " 'Ġin',\n",
       " 'Ġnew',\n",
       " 'Ġy',\n",
       " 'ork',\n",
       " ',',\n",
       " 'ĠI',\n",
       " 'Ġwas',\n",
       " 'Ġfore',\n",
       " 'warn',\n",
       " 'ed',\n",
       " 'Ġthat',\n",
       " 'Ġout',\n",
       " '-',\n",
       " 'of',\n",
       " '-',\n",
       " 'state',\n",
       " 'Ġinsurance',\n",
       " 'Ġcan',\n",
       " \"'t\",\n",
       " 'Ġbe',\n",
       " 'Ġaccepted',\n",
       " '?',\n",
       " 'Ġ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw)\n",
    "tokens_bpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also have heard of the BERT model. It uses a similar subword tokenisation method to BPE, called wordpiece. We can also test that out using the HuggingFace Transformers library. \n",
    "\n",
    "TODO: Use the code below to see if you can find some differences between BERT's wordpiece method and BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'register',\n",
       " 'my',\n",
       " 'vehicle',\n",
       " 'here',\n",
       " 'in',\n",
       " 'new',\n",
       " 'york',\n",
       " ',',\n",
       " 'i',\n",
       " 'was',\n",
       " 'fore',\n",
       " '##war',\n",
       " '##ned',\n",
       " 'that',\n",
       " 'out',\n",
       " '-',\n",
       " 'of',\n",
       " '-',\n",
       " 'state',\n",
       " 'insurance',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'be',\n",
       " 'accepted',\n",
       " '?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
