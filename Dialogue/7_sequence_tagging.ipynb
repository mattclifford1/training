{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sequence Tagging\n",
    "\n",
    "In this lab we will train a part-of-speech (POS) tagger using an HMM and then an RNN.\n",
    "\n",
    "### Outcomes\n",
    "* Be able to train and apply an HMM.\n",
    "* Understand what the steps of Viterbi are doing.\n",
    "* Recognise how to adapt Pytorch models to use RNN layers.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The first part of the notebook loads a POS dataset from the NLTK library.\n",
    "The second part implements and tests an HMM POS tagger.\n",
    "The third part adapts the neural network code from last week to train the RNN as a POS tagger.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the PoS Tagging Data\n",
    "\n",
    "To train our POS tagger, we will use the Brown corpus, which contains many different sources of English text (books, essays, newspaper articles, government documents...) collected and hand-labelled by linguists in 1967."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/mc15455/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/mc15455/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')  # download Brown corpus\n",
    "nltk.download('universal_tagset')  # download the universal tagset: the 17 PoS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 45872\n",
      "Number of test sentences: 11468\n",
      "Number of tagged words in the training set: 45872\n",
      "Number of tagged words in the test set: 11468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "nltk_data = list(brown.tagged_sents(tagset='universal'))\n",
    "train_set, test_set = train_test_split(\n",
    "    nltk_data,\n",
    "    train_size=0.80,\n",
    "    test_size=0.20,\n",
    "    random_state=101\n",
    ")\n",
    "print(f'Number of training sentences: {len(train_set)}')\n",
    "print(f'Number of test sentences: {len(test_set)}')\n",
    "\n",
    "# Separate the labels from the text\n",
    "train_toks = []\n",
    "train_tags = []\n",
    "for tagged_sentence in train_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "\n",
    "    train_toks.append(sentence_toks)\n",
    "    train_tags.append(sentence_tags)\n",
    "\n",
    "test_toks = []\n",
    "test_tags = []\n",
    "for tagged_sentence in test_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "    test_toks.append(sentence_toks)\n",
    "    test_tags.append(sentence_tags)\n",
    "\n",
    "print(f'Number of tagged words in the training set: {len(train_toks)}')\n",
    "print(f'Number of tagged words in the test set: {len(test_toks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45872/45872 [00:00<00:00, 2212320.77it/s]\n",
      "100%|██████████| 11468/11468 [00:00<00:00, 2117648.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary is 56057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Convert the tokens to IDs in a vocabulary ready for input to our models\n",
    "tok_encoder = LabelEncoder()\n",
    "\n",
    "all_train_words = [word for sentence in train_toks for word in sentence]\n",
    "all_test_words = [word for sentence in test_toks for word in sentence]\n",
    "all_words = all_train_words + all_test_words\n",
    "tok_encoder.fit(all_words)\n",
    "\n",
    "all_encoded = tok_encoder.transform(all_train_words)\n",
    "\n",
    "train_toks_encoded = []\n",
    "start_idx = 0\n",
    "for sentence in tqdm(train_toks):\n",
    "    train_toks_encoded.append(all_encoded[start_idx:start_idx+len(sentence)])\n",
    "    start_idx += len(sentence)\n",
    "\n",
    "all_encoded = tok_encoder.transform(all_test_words)\n",
    "\n",
    "test_toks_encoded = []\n",
    "start_idx = 0\n",
    "for sentence in tqdm(test_toks):\n",
    "    test_toks_encoded.append(all_encoded[start_idx:start_idx+len(sentence)])\n",
    "    start_idx += len(sentence)\n",
    "\n",
    "V = len(tok_encoder.classes_)\n",
    "print(f'Size of vocabulary is {V}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the tags from their names to numbers\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit([tag for sentence in train_tags for tag in sentence])\n",
    "train_tags_encoded = [tag_encoder.transform(sentence) for sentence in train_tags]\n",
    "test_tags_encoded = [tag_encoder.transform(sentence) for sentence in test_tags]\n",
    "\n",
    "num_tags = len(tag_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Implementing the HMM\n",
    "\n",
    "TODO: Count the state transitions and starting state occurrences in the training set and store the counts in the `transitions` and `start_states` matrices below. In `transitions`, rows correspond to states at time t-1, the columns to the following state at time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5  6 10  3  2  5  6  9  5  6  0]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(train_tags_encoded[0])\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45872/45872 [00:01<00:00, 43449.30it/s]\n"
     ]
    }
   ],
   "source": [
    "transitions = np.zeros((num_tags, num_tags))\n",
    "start_states = np.zeros(num_tags)\n",
    "\n",
    "for sentence_tags in tqdm(train_tags_encoded):\n",
    "    for i, tag in enumerate(sentence_tags):\n",
    "        if i==0:\n",
    "            start_states[tag] += 1\n",
    "        else:\n",
    "            transitions[tag, sentence_tags[i-1]] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: Normalise the transition and start state counts to estimate the conditional probabilities in the transition matrix and \\pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# normalise to make probabilities\n",
    "transition_probs = transitions/transitions.sum(axis=0)\n",
    "start_probs = start_states/start_states.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: Count the number of occurrences of each word type given each tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2288 12964 47199 49314 22229 19348 28778 33722 34478 48744   405]\n",
      "[ 5  6 10  3  2  5  6  9  5  6  0]\n"
     ]
    }
   ],
   "source": [
    "print(train_toks_encoded[0])\n",
    "print(train_tags_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45872it [00:01, 40098.19it/s]\n"
     ]
    }
   ],
   "source": [
    "observations = np.zeros((num_tags, V))\n",
    "\n",
    "for i, sentence_toks in tqdm(enumerate(train_toks_encoded)):\n",
    "    sentence_tags = train_tags_encoded[i]\n",
    "    for j, tok in enumerate(sentence_toks):\n",
    "        tag = sentence_tags[j]\n",
    "        observations[tag, tok] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: Normalise the observation counts to obtain the observation probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927092.0\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# normalise\n",
    "observation_probs = observations/observations.sum()\n",
    "print(observations.sum())\n",
    "print(observation_probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: check the implementation of viterbi below for errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs):\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_obs = observed_seq.shape[0]\n",
    "\n",
    "    # Initialise the V and backpointers\n",
    "    V = np.zeros((num_obs, num_tags))\n",
    "    backpointer = np.zeros((num_obs, num_tags))\n",
    "\n",
    "    # For the first data point in the sequence:\n",
    "    V[0, :] = start_probs * observation_probs[:, observed_seq[0]]\n",
    "\n",
    "    # Run Viterbi forward for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "\n",
    "        for state in range(num_tags):\n",
    "            # probabilities for all the sequences leading to this state at time t\n",
    "            seq_prob = V[t-1, :] * transition_probs[:, state]\n",
    "\n",
    "            # Choose the most likely sequence\n",
    "            max_seq_prob = np.max(seq_prob)\n",
    "            best_previous_state = np.argmax(seq_prob)\n",
    "\n",
    "            # Calculate the probability of the most likely sequence leading to this state at time t, including the current observation.\n",
    "            # Add eps to help with numerical issues.\n",
    "            V[t, state] = (max_seq_prob + eps) * (observation_probs[state, observed_seq[t]] + eps)\n",
    "\n",
    "            backpointer[t, state] = best_previous_state\n",
    "\n",
    "    t = num_obs - 1\n",
    "\n",
    "    # Initialise the sequence of predicted states\n",
    "    state_seq = np.zeros(num_obs, dtype=int)\n",
    "\n",
    "    # Get the most likely final state:\n",
    "    state_seq[t] = np.argmax(V[t, :])\n",
    "\n",
    "    # Backtrack until the first observation\n",
    "    for t in range(len(observed_seq)-1, 0, -1):\n",
    "        state_seq[t-1] = backpointer[t, state_seq[t]]\n",
    "\n",
    "    return state_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: Use the viterbi function to estimate the most likely sequence of states on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9618   393 51924 55190 22851 55278 20308 27893 35400 19348 47985 41065\n",
      " 13432  5649 44651 55334 26936 35400 19348 31420 44345 35400  2012   405]\n",
      "[ 3  0  5  0  6 10  3 10  2  5  6  2  1  6  6  5 10  2  5  1  6  2  7  0]\n",
      "[ 3  0  5 11  6 10  3 10  2  5  6  2  1  1  6  5 10  2  5  1  6  2  7  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11468/11468 [00:39<00:00, 292.67it/s]\n"
     ]
    }
   ],
   "source": [
    "observed_seq = np.array(test_toks_encoded[0])\n",
    "state_seq = viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs)\n",
    "print(test_toks_encoded[0])\n",
    "print(state_seq)\n",
    "print(test_tags_encoded[0])\n",
    "predictions = []\n",
    "for test_tok in tqdm(test_toks_encoded):\n",
    "    predictions.append(viterbi(test_tok, num_tags, start_probs, transition_probs, observation_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11468/11468 [00:01<00:00, 11119.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'My', 'God', ',', \"I'm\", 'shot', \"''\", '!', '!']\n",
      "['.', 'DET', 'NOUN', '.', 'PRT', 'VERB', '.', '.', '.']\n",
      "['.' 'NOUN' 'NOUN' '.' 'PRT' 'NOUN' '.' '.' '.']\n",
      "['She', 'thought', 'she', 'was', 'bigger', 'than', 'we', 'are', 'because', 'she', 'came', 'from', 'Torino', \"''\", '.']\n",
      "['PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP', 'PRON', 'VERB', 'ADP', 'PRON', 'VERB', 'ADP', 'NOUN', '.', '.']\n",
      "['PRON' 'VERB' 'PRON' 'VERB' 'ADJ' 'ADP' 'PRON' 'VERB' 'ADP' 'PRON' 'VERB'\n",
      " 'ADP' 'NOUN' '.' '.']\n",
      "['Meanwhile', ',', 'I', 'reloaded', 'my', 'gun', ',', 'as', 'the', 'other', 'men', 'were', 'doing', '.']\n",
      "['ADV', '.', 'PRON', 'VERB', 'DET', 'NOUN', '.', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', '.']\n",
      "['ADV' '.' 'PRON' 'VERB' 'DET' 'NOUN' '.' 'ADP' 'DET' 'ADJ' 'NOUN' 'VERB'\n",
      " 'VERB' '.']\n",
      "['The', 'difficulty', 'was', 'that', 'each', 'day', 'seemed', 'to', 'produce', 'its', 'quota', 'of', 'details', 'which', 'must', 'be', 'cleaned', 'up', 'immediately', '.']\n",
      "['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'VERB', 'PRT', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'DET', 'VERB', 'VERB', 'VERB', 'PRT', 'ADV', '.']\n",
      "['DET' 'NOUN' 'VERB' 'ADP' 'DET' 'NOUN' 'VERB' 'ADP' 'NOUN' 'DET' 'NOUN'\n",
      " 'ADP' 'NOUN' 'DET' 'VERB' 'VERB' 'VERB' 'PRT' 'ADV' '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the sequence of tag IDs to tag names\n",
    "predicted_tags = []\n",
    "for sequence in tqdm(predictions):\n",
    "    predicted_tags.append(tag_encoder.inverse_transform(sequence))\n",
    "\n",
    "# print some examples:\n",
    "examples = [2, 334, 4983, 2389]\n",
    "for eg in examples:\n",
    "    print(test_toks[eg])\n",
    "    print(test_tags[eg])\n",
    "    print(predicted_tags[eg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9080478428022213\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "all_predictions = [tag for sentence in predictions for tag in sentence]\n",
    "all_targets = [tag for sentence in test_tags_encoded for tag in sentence]\n",
    "\n",
    "acc = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. POS Tagging with an RNN\n",
    "The code below is adapted from last week to first pad the sequences, then format them into DataLoader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sequence_length = 40  # truncate all docs longer than this. Pad all docs shorter than this.\n",
    "\n",
    "def pad_sequence(sequence):\n",
    "    # pad with -1s\n",
    "    if len(sequence) >= sequence_length:\n",
    "        sequence = sequence[:sequence_length]\n",
    "    else:\n",
    "        sequence = np.concatenate((np.zeros(sequence_length-len(sequence)) + V, sequence))\n",
    "    return sequence\n",
    "\n",
    "padded_train_toks_encoded = [pad_sequence(toks)[None, :] for toks in train_toks_encoded]\n",
    "padded_train_tags_encoded = [pad_sequence(tags)[None, :] for tags in train_tags_encoded]\n",
    "\n",
    "padded_test_toks_encoded = [pad_sequence(toks)[None, :] for toks in test_toks_encoded]\n",
    "padded_test_tags_encoded = [pad_sequence(tags)[None, :] for tags in test_tags_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(inputs, labels):\n",
    "    inputs = np.concatenate(inputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "    # convert from array to tensor\n",
    "    input_tensor = torch.from_numpy(inputs).long()\n",
    "    label_tensor = torch.from_numpy(labels).long()\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "train_loader = convert_to_data_loader(padded_train_toks_encoded, padded_train_tags_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = convert_to_data_loader(padded_test_toks_encoded, padded_test_tags_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "TODO: Complete the code below to change the hidden layer to a single RNN layer. See [the documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_classes):\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "\n",
    "        ### COMPLETE YOUR CODE HERE\n",
    "        self.hidden_layer = torch.nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        ###\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Full connection layer\n",
    "\n",
    "    def forward (self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        ### COMPLETE THE CODE HERE TO USE THE HIDDEN RNN LAYER\n",
    "        h = self.hidden_layer(embedded_words)[0]  # (batch_size, seq_length, hidden_size)\n",
    "        ###\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, seq_length, num_classes)\n",
    "        output = torch.transpose(output, 1, 2)              # (batch_size, num_classes, seq_length)\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we can run the code below to test the RNN model.\n",
    "\n",
    "TODO: What is wrong with comparing the RNN tagger's performance computed here with that of the HMM? Hint: all the sequences are length 40.\n",
    "TODO: Can you fix the accuracy computations to make them comparable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 25  # number of dimensions for embeddings\n",
    "hidden_size = 32 # number of hidden units\n",
    "\n",
    "ff_classifier_model = FFTextClassifier(V+1, embedding_size, hidden_size, num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_nn(num_epochs, model, train_dataloader, dev_dataloader, loss_fn, optimizer):\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        # Track performance on the training set as we are learning...\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        train_losses = []\n",
    "\n",
    "        model.train()  # Put the model in training mode.\n",
    "\n",
    "        for i, (batch_input_ids, batch_labels) in enumerate(train_dataloader):\n",
    "            # Iterate over each batch of data\n",
    "            # print(f'batch no. = {i}')\n",
    "\n",
    "            optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "            # Use the model to perform forward inference on the input data.\n",
    "            # This will run the forward() function.\n",
    "            output = model(batch_input_ids)\n",
    "\n",
    "            # Compute the loss for the current batch of data\n",
    "            batch_loss = loss_fn(output, batch_labels)\n",
    "\n",
    "            # Perform back propagation to compute the gradients with respect to each weight\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Update the weights using the compute gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the loss from this sample to keep track of progress.\n",
    "            train_losses.append(batch_loss.item())\n",
    "\n",
    "            # Count correct labels so we can compute accuracy on the training set\n",
    "            predicted_labels = output.argmax(1)\n",
    "            total_correct += (predicted_labels == batch_labels).sum().item()\n",
    "            total_trained += batch_labels.size(0) * sequence_length\n",
    "\n",
    "        train_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Training Loss: {:.4f}\".format(np.mean(train_losses)),\n",
    "              \"Training Accuracy: {:.4f}%\".format(train_accuracy))\n",
    "\n",
    "        model.eval()  # Switch model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_trained = 0\n",
    "        dev_losses = []\n",
    "\n",
    "        for dev_input_ids, dev_labels in dev_dataloader:\n",
    "            dev_output = model(dev_input_ids)\n",
    "            dev_loss = loss_fn(dev_output, dev_labels)\n",
    "\n",
    "            # Save the loss on the dev set\n",
    "            dev_losses.append(dev_loss.item())\n",
    "\n",
    "            # Count the number of correct predictions\n",
    "            predicted_labels = dev_output.argmax(1)\n",
    "            total_correct += (predicted_labels == dev_labels).sum().item()\n",
    "            total_trained += dev_labels.size(0) * sequence_length\n",
    "\n",
    "        dev_accuracy = total_correct/total_trained*100\n",
    "\n",
    "        print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "              \"Validation Loss: {:.4f}\".format(np.mean(dev_losses)),\n",
    "              \"Validation Accuracy: {:.4f}%\".format(dev_accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Training Loss: 0.1683 Training Accuracy: 45.7665%\n",
      "Epoch: 1/10 Validation Loss: 0.2001 Validation Accuracy: 45.5792%\n",
      "Epoch: 2/10 Training Loss: 0.1562 Training Accuracy: 45.9463%\n",
      "Epoch: 2/10 Validation Loss: 0.1919 Validation Accuracy: 45.7050%\n",
      "Epoch: 3/10 Training Loss: 0.1455 Training Accuracy: 46.1062%\n",
      "Epoch: 3/10 Validation Loss: 0.1851 Validation Accuracy: 45.8197%\n",
      "Epoch: 4/10 Training Loss: 0.1362 Training Accuracy: 46.2454%\n",
      "Epoch: 4/10 Validation Loss: 0.1794 Validation Accuracy: 45.8984%\n",
      "Epoch: 5/10 Training Loss: 0.1279 Training Accuracy: 46.3631%\n",
      "Epoch: 5/10 Validation Loss: 0.1752 Validation Accuracy: 45.9749%\n",
      "Epoch: 6/10 Training Loss: 0.1204 Training Accuracy: 46.4796%\n",
      "Epoch: 6/10 Validation Loss: 0.1718 Validation Accuracy: 46.0462%\n",
      "Epoch: 7/10 Training Loss: 0.1139 Training Accuracy: 46.5775%\n",
      "Epoch: 7/10 Validation Loss: 0.1684 Validation Accuracy: 46.1277%\n",
      "Epoch: 8/10 Training Loss: 0.1080 Training Accuracy: 46.6738%\n",
      "Epoch: 8/10 Validation Loss: 0.1658 Validation Accuracy: 46.1896%\n",
      "Epoch: 9/10 Training Loss: 0.1027 Training Accuracy: 46.7607%\n",
      "Epoch: 9/10 Validation Loss: 0.1640 Validation Accuracy: 46.2387%\n",
      "Epoch: 10/10 Training Loss: 0.0979 Training Accuracy: 46.8349%\n",
      "Epoch: 10/10 Validation Loss: 0.1628 Validation Accuracy: 46.2703%\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=V)\n",
    "optimizer = optim.Adam(ff_classifier_model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "trained_model = train_nn(num_epochs, ff_classifier_model, train_loader, test_loader, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "def test_nn(trained_model, test_loader, loss_fn):\n",
    "\n",
    "    trained_model.eval()\n",
    "\n",
    "    test_losses = []\n",
    "    correct = 0  # count the number of correct classification labels\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        test_output = trained_model(inputs)\n",
    "        loss = loss_fn(test_output, labels)\n",
    "        test_losses.append(loss.item())\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "\n",
    "        count_correct = torch.sum(predicted_labels == labels).item()\n",
    "        correct += count_correct\n",
    "\n",
    "    accuracy = correct/(len(test_loader.dataset)*sequence_length*100)\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
    "    # print(predicted)\n",
    "\n",
    "test_nn(trained_model, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
